{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic semantic segmentation using average unpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packs loaded.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import _pickle as pkl\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.training import moving_averages\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "%matplotlib inline  \n",
    "print (\"Packs loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset for semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 train images\n",
      "367 train annotations\n",
      "233 test images\n",
      "233 test annotations\n"
     ]
    }
   ],
   "source": [
    "# Location of the files\n",
    "camvidpath = 'data/CamVid/'\n",
    "# Training data\n",
    "path1 = os.getcwd() + '/' + camvidpath + 'train/'\n",
    "path2 = os.getcwd() + '/' + camvidpath + 'trainannot/'\n",
    "trainimglist = glob.glob(path1 + '/*.png')\n",
    "trainannotlist = glob.glob(path2 + '/*.png')\n",
    "print (\"%d train images\" % (len(trainimglist)))\n",
    "print (\"%d train annotations\" % (len(trainannotlist)))\n",
    "\n",
    "# Test data\n",
    "path1 = os.getcwd() + '/' + camvidpath + 'test/'\n",
    "path2 = os.getcwd() + '/' + camvidpath + 'testannot/'\n",
    "testimglist = glob.glob(path1 + '/*.png')\n",
    "testannotlist = glob.glob(path2 + '/*.png')\n",
    "print (\"%d test images\" % (len(testimglist)))\n",
    "print (\"%d test annotations\" % (len(testannotlist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train / test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data process done.\n",
      "Test data process done.\n"
     ]
    }
   ],
   "source": [
    "height = 128\n",
    "width = 128\n",
    "nrclass = 22\n",
    "trainData = None\n",
    "trainLabel = None\n",
    "trainLabelOneHot = None\n",
    "trainlen = len(trainimglist)\n",
    "testData = None\n",
    "testLabel = None\n",
    "testLabelOneHot = None\n",
    "testlen = len(testimglist)\n",
    "def DenseToOneHot(labels_dense, num_classes):\n",
    "    # Convert class labels from scalars to one-hot vectors. \n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\"\"\" Train data process \"\"\" \n",
    "for (f1, f2, i) in zip(trainimglist, trainannotlist, range(trainlen)):\n",
    "    # print (\"[%02d/%02d]f1: %sf2: %s\" % (i, trainlen, f1, f2))\n",
    "    # Train image\n",
    "    img1 = Image.open(f1)\n",
    "    img1 = img1.resize((height, width))\n",
    "    rgb  = np.array(img1).reshape(1, height, width, 3)\n",
    "    # Train label\n",
    "    img2 = Image.open(f2)\n",
    "    img2 = img2.resize((height, width), Image.NEAREST)\n",
    "    label = np.array(img2).reshape(1, height, width, 1)\n",
    "    # Stack images and labels\n",
    "    if i == 0: \n",
    "        trainData = rgb\n",
    "        trainLabel = label\n",
    "    else:\n",
    "        trainData = np.concatenate((trainData, rgb), axis=0)\n",
    "        trainLabel = np.concatenate((trainLabel, label), axis=0)\n",
    "ntrain = len(trainData)\n",
    "# Onehot-coded label\n",
    "trainLabelOneHot = np.zeros((trainLabel.shape[0], trainLabel.shape[1]                             , trainLabel.shape[2], nrclass))\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        single = trainLabel[:, row, col, 0]\n",
    "        oneHot = DenseToOneHot(single, nrclass) # (367,) => (367, 22)\n",
    "        trainLabelOneHot[:, row, col, :] = oneHot\n",
    "print (\"Train data process done.\")     \n",
    "\n",
    "\"\"\" Test data process \"\"\" \n",
    "for (f1, f2, i) in zip(testimglist, testannotlist, range(testlen)):\n",
    "    # print (\"[%02d/%02d]f1: %sf2: %s\" % (i, testlen, f1, f2))\n",
    "    # Train image\n",
    "    img1 = Image.open(f1)\n",
    "    img1 = img1.resize((height, width))\n",
    "    rgb  = np.array(img1).reshape(1, height, width, 3)\n",
    "    # Train label\n",
    "    img2 = Image.open(f2)\n",
    "    img2 = img2.resize((height, width), Image.NEAREST)\n",
    "    label = np.array(img2).reshape(1, height, width, 1)\n",
    "    # Stack images and labels\n",
    "    if i == 0: \n",
    "        testData = rgb\n",
    "        testLabel = label\n",
    "    else:\n",
    "        testData = np.concatenate((testData, rgb), axis=0)\n",
    "        testLabel = np.concatenate((testLabel, label), axis=0)\n",
    "# Onehot-coded label\n",
    "testLabelOneHot = np.zeros((testLabel.shape[0], testLabel.shape[1], testLabel.shape[2], nrclass))\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        single = testLabel[:, row, col, 0]\n",
    "        oneHot = DenseToOneHot(single, nrclass) # (367,) => (367, 22)\n",
    "        testLabelOneHot[:, row, col, :] = oneHot\n",
    "print (\"Test data process done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'trainData' is (367, 128, 128, 3)\n",
      "Shape of 'trainLabel' is (367, 128, 128, 1)\n",
      "Shape of 'trainLabelOneHot' is (367, 128, 128, 22)\n",
      "Shape of 'testData' is (233, 128, 128, 3)\n",
      "Shape of 'testLabel' is (233, 128, 128, 1)\n",
      "Shape of 'testLabelOneHot' is (233, 128, 128, 22)\n"
     ]
    }
   ],
   "source": [
    "print (\"Shape of 'trainData' is %s\" % (trainData.shape,))\n",
    "print (\"Shape of 'trainLabel' is %s\" % (trainLabel.shape,))\n",
    "print (\"Shape of 'trainLabelOneHot' is %s\" % (trainLabelOneHot.shape,))\n",
    "print (\"Shape of 'testData' is %s\" % (testData.shape,))\n",
    "print (\"Shape of 'testLabel' is %s\" % (testLabel.shape,))\n",
    "print (\"Shape of 'testLabelOneHot' is %s\" % (testLabelOneHot.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable ce1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-5-04a579335913>\", line 12, in <module>\n    'ce1': tf.get_variable(\"ce1\", shape = [ksize, ksize, 3, fsize], initializer = initfun) ,\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-04a579335913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# initfun = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m weights = {\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m'ce1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ce1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitfun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m'ce2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ce2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitfun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m'ce3'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ce3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1329\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1330\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 743\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable ce1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-5-04a579335913>\", line 12, in <module>\n    'ce1': tf.get_variable(\"ce1\", shape = [ksize, ksize, 3, fsize], initializer = initfun) ,\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# Define functions\n",
    "x = tf.placeholder(tf.float32, [None, height, width, 3])\n",
    "y = tf.placeholder(tf.float32, [None, height, width, nrclass])\n",
    "keepprob = tf.placeholder(tf.float32)\n",
    "# Kernels\n",
    "ksize = 5\n",
    "fsize = 64\n",
    "initstdev = 0.01\n",
    "initfun = tf.random_normal_initializer(mean=0.0, stddev=initstdev)\n",
    "# initfun = None\n",
    "weights = {\n",
    "    'ce1': tf.get_variable(\"ce1\", shape = [ksize, ksize, 3, fsize], initializer = initfun) ,\n",
    "    'ce2': tf.get_variable(\"ce2\", shape = [ksize, ksize, fsize, fsize], initializer = initfun) ,\n",
    "    'ce3': tf.get_variable(\"ce3\", shape = [ksize, ksize, fsize, fsize], initializer = initfun),\n",
    "    'ce4': tf.get_variable(\"ce4\", shape = [ksize, ksize, fsize, fsize], initializer = initfun),\n",
    "    'cd4': tf.get_variable(\"cd4\", shape = [ksize, ksize, fsize, fsize], initializer = initfun),\n",
    "    'cd3': tf.get_variable(\"cd3\", shape = [ksize, ksize, fsize, fsize], initializer = initfun),\n",
    "    'cd2': tf.get_variable(\"cd2\", shape = [ksize, ksize, fsize, fsize], initializer = initfun),\n",
    "    'cd1': tf.get_variable(\"cd1\", shape = [ksize, ksize, fsize, fsize], initializer = initfun),\n",
    "    'dense_inner_prod': tf.get_variable(\"dense_inner_prod\", shape= [1, 1, fsize, nrclass]\n",
    "                                       , initializer = initfun) # <= 1x1conv\n",
    "}\n",
    "biases = {\n",
    "    'be1': tf.get_variable(\"be1\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'be2': tf.get_variable(\"be2\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'be3': tf.get_variable(\"be3\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'be4': tf.get_variable(\"be4\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'bd4': tf.get_variable(\"bd4\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'bd3': tf.get_variable(\"bd3\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'bd2': tf.get_variable(\"bd2\", shape = [fsize], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'bd1': tf.get_variable(\"bd1\", shape = [fsize], initializer = tf.constant_initializer(value=0.0))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeconvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "# input : [m, h, w, c]\n",
    "def Unpooling(inputOrg, size, mask=None):\n",
    "    # m, c, h, w order\n",
    "    m = size[0]\n",
    "    h = size[1]\n",
    "    w = size[2]\n",
    "    c = size[3]\n",
    "    input = tf.transpose(inputOrg, [0, 3, 1, 2])\n",
    "    x = tf.reshape(input, [-1, 1])\n",
    "    k = np.float32(np.array([1.0, 1.0]).reshape([1,-1]))\n",
    "    output = tf.matmul(x, k)\n",
    "    output = tf.reshape(output,[-1, c, h, w * 2])\n",
    "    # m, c, w, h\n",
    "    xx = tf.transpose(output, [0, 1, 3, 2])\n",
    "    xx = tf.reshape(xx,[-1, 1])\n",
    "    output = tf.matmul(xx, k)\n",
    "    # m, c, w, h\n",
    "    output = tf.reshape(output, [-1, c, w * 2, h * 2])\n",
    "    output = tf.transpose(output, [0, 3, 2, 1])\n",
    "    outshape = tf.stack([m, h * 2, w * 2, c])\n",
    "    if mask != None:\n",
    "        dense_mask = tf.sparse_to_dense(mask, outshape, output, 0)\n",
    "        return output, dense_mask\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "# DeconvNet Model\n",
    "def Model(_X, _W, _b, _keepprob):\n",
    "    use_bias = 1\n",
    "    # Encoder 128x128\n",
    "    encoder1 = tf.nn.conv2d(_X, _W['ce1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        encoder1 = tf.nn.bias_add(encoder1, _b['be1'])\n",
    "    mean, var = tf.nn.moments(encoder1, [0, 1, 2])\n",
    "    encoder1 = tf.nn.batch_normalization(encoder1, mean, var, 0, 1, 0.0001)\n",
    "    encoder1 = tf.nn.relu(encoder1)\n",
    "    encoder1 = tf.nn.max_pool(encoder1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    encoder1 = tf.nn.dropout(encoder1, _keepprob)\n",
    "    # 64x64\n",
    "    encoder2 = tf.nn.conv2d(encoder1, _W['ce2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        encoder2 = tf.nn.bias_add(encoder2, _b['be2'])\n",
    "    mean, var = tf.nn.moments(encoder1, [0, 1, 2])\n",
    "    encoder2 = tf.nn.batch_normalization(encoder2, mean, var, 0, 1, 0.0001)\n",
    "    encoder2 = tf.nn.relu(encoder2)\n",
    "    encoder2 = tf.nn.max_pool(encoder2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    encoder2 = tf.nn.dropout(encoder2, _keepprob)\n",
    "    # 32x32\n",
    "    encoder3 = tf.nn.conv2d(encoder2, _W['ce3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        encoder3 = tf.nn.bias_add(encoder3, _b['be3'])\n",
    "    mean, var = tf.nn.moments(encoder3, [0, 1, 2])\n",
    "    encoder3 = tf.nn.batch_normalization(encoder3, mean, var, 0, 1, 0.0001)\n",
    "    encoder3 = tf.nn.relu(encoder3)\n",
    "    encoder3 = tf.nn.max_pool(encoder3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    encoder3 = tf.nn.dropout(encoder3, _keepprob)\n",
    "    # 16x16\n",
    "    encoder4 = tf.nn.conv2d(encoder3, _W['ce4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        encoder4 = tf.nn.bias_add(encoder4, _b['be4'])\n",
    "    mean, var = tf.nn.moments(encoder4, [0, 1, 2])\n",
    "    encoder4 = tf.nn.batch_normalization(encoder4, mean, var, 0, 1, 0.0001)\n",
    "    encoder4 = tf.nn.relu(encoder4)\n",
    "    encoder4 = tf.nn.max_pool(encoder4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    encoder4 = tf.nn.dropout(encoder4, _keepprob)\n",
    "    # 8x8\n",
    "\n",
    "    # Decoder 8x8 (128/16 = 8) fsize: 64\n",
    "    decoder4 = Unpooling(encoder4, [tf.shape(_X)[0], int(height / 16), int(width / 16), fsize])\n",
    "    decoder4 = tf.nn.conv2d_transpose(decoder4, _W['cd4']\n",
    "                , tf.stack([tf.shape(_X)[0], ksize, ksize, fsize])\n",
    "                , strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        decoder4 = tf.nn.bias_add(decoder4, _b['bd4'])\n",
    "    mean, var = tf.nn.moments(decoder4, [0, 1, 2])\n",
    "    decoder4 = tf.nn.batch_normalization(decoder4, mean, var, 0, 1, 0.0001)\n",
    "    decoder4 = tf.nn.relu(decoder4)\n",
    "    decoder4 = tf.nn.dropout(decoder4, _keepprob)\n",
    "    # 16x16\n",
    "    decoder3 = Unpooling(encoder3, [tf.shape(_X)[0], int(height/8), int(width/8), fsize])\n",
    "    decoder3 = tf.nn.conv2d(decoder3, _W['cd3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    if use_bias:\n",
    "        decoder3 = tf.nn.bias_add(decoder3, _b['bd3'])\n",
    "    mean, var = tf.nn.moments(decoder3, [0, 1, 2])\n",
    "    decoder3 = tf.nn.batch_normalization(decoder3, mean, var, 0, 1, 0.0001)\n",
    "    decoder3 = tf.nn.relu(decoder3)\n",
    "    decoder3 = tf.nn.dropout(decoder3, _keepprob)\n",
    "    # 32x32\n",
    "    decoder2 = Unpooling(decoder3, [tf.shape(_X)[0], int(height/4), int(width/4), fsize])\n",
    "    decoder2 = tf.nn.conv2d(decoder2, _W['cd2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        decoder2 = tf.nn.bias_add(decoder2, _b['bd2'])\n",
    "    mean, var = tf.nn.moments(decoder2, [0, 1, 2])\n",
    "    decoder2 = tf.nn.batch_normalization(decoder2, mean, var, 0, 1, 0.0001)\n",
    "    decoder2 = tf.nn.relu(decoder2)\n",
    "    decoder2 = tf.nn.dropout(decoder2, _keepprob)\n",
    "    # 64x64\n",
    "    decoder1 = Unpooling(decoder2, [tf.shape(_X)[0], int(height / 2), int(width / 2), fsize])\n",
    "    decoder1 = tf.nn.conv2d(decoder1, _W['cd1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if use_bias:\n",
    "        decoder1 = tf.nn.bias_add(decoder1, _b['bd1'])\n",
    "    mean, var = tf.nn.moments(decoder1, [0, 1, 2])\n",
    "    decoder1 = tf.nn.batch_normalization(decoder1, mean, var, 0, 1, 0.0001)\n",
    "    decoder1 = tf.nn.relu(decoder1)\n",
    "    decoder1 = tf.nn.dropout(decoder1, _keepprob)\n",
    "    # 128x128\n",
    "    output = tf.nn.conv2d(decoder1, _W['dense_inner_prod'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return output\n",
    "\n",
    "print (\"Network ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready\n"
     ]
    }
   ],
   "source": [
    "pred = Model(x, weights, biases, keepprob)\n",
    "lin_pred = tf.reshape(pred, shape=[-1, nrclass])\n",
    "lin_y = tf.reshape(y, shape=[-1, nrclass])\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = lin_pred, labels = lin_y))\n",
    "# Class label\n",
    "predmax = tf.argmax(pred, 3)\n",
    "ymax = tf.argmax(y, 3)\n",
    "# Accuracy\n",
    "corr = tf.equal(tf.argmax(y,3), tf.argmax(pred, 3)) \n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "# Optimizer\n",
    "optm = tf.train.AdamOptimizer(0.0001).minimize(cost)\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "\n",
    "print (\"Functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real optimization starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "checkpoint: None\n",
      "Couldn't find checkpoint to restore from. Starting over.\n",
      "[00/20] trainLoss: 3.0506 trainAcc: 0.25 valLoss: 3.0404 valAcc: 0.34\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of nets/semseg_basic/progress doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: nets/semseg_basic; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, bd1, bd1/Adam, bd1/Adam_1, bd1/Adam_2, bd1/Adam_3, bd2, bd2/Adam, bd2/Adam_1, bd2/Adam_2, bd2/Adam_3, bd3, bd3/Adam, bd3/Adam_1, bd3/Adam_2, bd3/Adam_3, bd4, be1, be1/Adam, be1/Adam_1, be1/Adam_2, be1/Adam_3, be2, be2/Adam, be2/Adam_1, be2/Adam_2, be2/Adam_3, be3, be3/Adam, be3/Adam_1, be3/Adam_2, be3/Adam_3, be4, beta1_power, beta1_power_1, beta2_power, beta2_power_1, cd1, cd1/Adam, cd1/Adam_1, cd1/Adam_2, cd1/Adam_3, cd2, cd2/Adam, cd2/Adam_1, cd2/Adam_2, cd2/Adam_3, cd3, cd3/Adam, cd3/Adam_1, cd3/Adam_2, cd3/Adam_3, cd4, ce1, ce1/Adam, ce1/Adam_1, ce1/Adam_2, ce1/Adam_3, ce2, ce2/Adam, ce2/Adam_1, ce2/Adam_2, ce2/Adam_3, ce3, ce3/Adam, ce3/Adam_1, ce3/Adam_2, ce3/Adam_3, ce4, dense_inner_prod, dense_inner_prod/Adam, dense_inner_prod/Adam_1, dense_inner_prod/Adam_2, dense_inner_prod/Adam_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: nets/semseg_basic; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, bd1, bd1/Adam, bd1/Adam_1, bd1/Adam_2, bd1/Adam_3, bd2, bd2/Adam, bd2/Adam_1, bd2/Adam_2, bd2/Adam_3, bd3, bd3/Adam, bd3/Adam_1, bd3/Adam_2, bd3/Adam_3, bd4, be1, be1/Adam, be1/Adam_1, be1/Adam_2, be1/Adam_3, be2, be2/Adam, be2/Adam_1, be2/Adam_2, be2/Adam_3, be3, be3/Adam, be3/Adam_1, be3/Adam_2, be3/Adam_3, be4, beta1_power, beta1_power_1, beta2_power, beta2_power_1, cd1, cd1/Adam, cd1/Adam_1, cd1/Adam_2, cd1/Adam_3, cd2, cd2/Adam, cd2/Adam_1, cd2/Adam_2, cd2/Adam_3, cd3, cd3/Adam, cd3/Adam_1, cd3/Adam_2, cd3/Adam_3, cd4, ce1, ce1/Adam, ce1/Adam_1, ce1/Adam_2, ce1/Adam_3, ce2, ce2/Adam, ce2/Adam_1, ce2/Adam_2, ce2/Adam_3, ce3, ce3/Adam, ce3/Adam_1, ce3/Adam_2, ce3/Adam_3, ce4, dense_inner_prod, dense_inner_prod/Adam, dense_inner_prod/Adam_1, dense_inner_prod/Adam_2, dense_inner_prod/Adam_3)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-998d74c9b053>\", line 5, in <module>\n    saver = tf.train.Saver()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 278, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 194, in save_op\n    tensors)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/tuan.cao/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): nets/semseg_basic; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, bd1, bd1/Adam, bd1/Adam_1, bd1/Adam_2, bd1/Adam_3, bd2, bd2/Adam, bd2/Adam_1, bd2/Adam_2, bd2/Adam_3, bd3, bd3/Adam, bd3/Adam_1, bd3/Adam_2, bd3/Adam_3, bd4, be1, be1/Adam, be1/Adam_1, be1/Adam_2, be1/Adam_3, be2, be2/Adam, be2/Adam_1, be2/Adam_2, be2/Adam_3, be3, be3/Adam, be3/Adam_1, be3/Adam_2, be3/Adam_3, be4, beta1_power, beta1_power_1, beta2_power, beta2_power_1, cd1, cd1/Adam, cd1/Adam_1, cd1/Adam_2, cd1/Adam_3, cd2, cd2/Adam, cd2/Adam_1, cd2/Adam_2, cd2/Adam_3, cd3, cd3/Adam, cd3/Adam_1, cd3/Adam_2, cd3/Adam_3, cd4, ce1, ce1/Adam, ce1/Adam_1, ce1/Adam_2, ce1/Adam_3, ce2, ce2/Adam, ce2/Adam_1, ce2/Adam_2, ce2/Adam_3, ce3, ce3/Adam, ce3/Adam_1, ce3/Adam_2, ce3/Adam_3, ce4, dense_inner_prod, dense_inner_prod/Adam, dense_inner_prod/Adam_1, dense_inner_prod/Adam_2, dense_inner_prod/Adam_3)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-998d74c9b053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresumeTraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nets/semseg_basic/progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1668\u001b[0m                   save_path))\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of nets/semseg_basic/progress doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "resumeTraining = True\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.latest_checkpoint(\"nets/semseg_basic\")\n",
    "    print (\"checkpoint: %s\" % (checkpoint))\n",
    "    if resumeTraining == False:\n",
    "        print (\"Start from scratch\")\n",
    "    elif  checkpoint:\n",
    "        print (\"Restoring from checkpoint\", checkpoint)\n",
    "        saver.restore(sess, checkpoint)\n",
    "    else:\n",
    "        print (\"Couldn't find checkpoint to restore from. Starting over.\")\n",
    "    \n",
    "    for epoch_i in range(n_epochs):\n",
    "        trainLoss = []; trainAcc = []\n",
    "        num_batch = int(ntrain/batch_size)+1\n",
    "        for _ in range(num_batch):\n",
    "            randidx = np.random.randint(ntrain, size=batch_size)\n",
    "            batchData = trainData[randidx]\n",
    "            batchLabel = trainLabelOneHot[randidx]\n",
    "            sess.run(optm, feed_dict={x: batchData, y: batchLabel, keepprob: 0.7}) # <== Optm is done here!\n",
    "            trainLoss.append(sess.run(cost, feed_dict={x: batchData, y: batchLabel, keepprob: 1.}))\n",
    "            trainAcc.append(sess.run(accr, feed_dict={x: batchData, y: batchLabel, keepprob: 1.}))\n",
    "        # Average loss and accuracy\n",
    "        trainLoss = np.mean(trainLoss)\n",
    "        trainAcc = np.mean(trainAcc)\n",
    "        # Run test\n",
    "        valLoss = sess.run(cost, feed_dict={x: testData, y: testLabelOneHot, keepprob: 1.})\n",
    "        valAcc = sess.run(accr, feed_dict={x: testData, y: testLabelOneHot, keepprob: 1.})\n",
    "        print (\"[%02d/%02d] trainLoss: %.4f trainAcc: %.2f valLoss: %.4f valAcc: %.2f\" \n",
    "               % (epoch_i, n_epochs, trainLoss, trainAcc, valLoss, valAcc))\n",
    "        # Save snapshot\n",
    "        if resumeTraining and epoch_i % 10 == 0:\n",
    "            # Save\n",
    "            saver.save(sess, 'model_checkpoint', global_step = epoch_i)\n",
    "            # Train data\n",
    "            index = np.random.randint(trainData.shape[0])\n",
    "            refimg = trainData[index, :, :, :].reshape(height, width, 3)\n",
    "            batchData = trainData[index:index+1]\n",
    "            batchLabel = trainLabelOneHot[index:index+1]\n",
    "            predMaxOut = sess.run(predmax, feed_dict={x: batchData, y: batchLabel, keepprob:1.})\n",
    "            yMaxOut = sess.run(ymax, feed_dict={x: batchData, y: batchLabel, keepprob:1.})\n",
    "            gtimg = yMaxOut[0, :, :].reshape(height, width)\n",
    "            errimg = gtimg - predMaxOut[0, :, :].reshape(height, width);\n",
    "            # Plot\n",
    "            xs = np.linspace(0, 140, 128); ys = np.linspace(140, 0, 128)\n",
    "            plt.figure(figsize=(10, 10)) \n",
    "            plt.subplot(2, 2, 1); plt.imshow(refimg); plt.title('Input')\n",
    "            plt.subplot(2, 2, 2); plt.pcolor(xs, ys, gtimg, vmin=0, vmax=nrclass); plt.title('Ground truth')\n",
    "            plt.subplot(2, 2, 3); plt.pcolor(xs, ys, predMaxOut[0, :, :].reshape(height, width), vmin=0, vmax=nrclass); plt.title('[Training] Prediction')\n",
    "            plt.subplot(2, 2, 4); plt.imshow(np.abs(errimg) > 0.5); plt.title('Error')\n",
    "            plt.show() \n",
    "            # Test data\n",
    "            index = np.random.randint(testData.shape[0])\n",
    "            batchData = testData[index:index+1]\n",
    "            batchLabel = testLabelOneHot[index:index+1]\n",
    "            predMaxOut = sess.run(predmax, feed_dict={x: batchData, y: batchLabel, keepprob:1.})\n",
    "            yMaxOut = sess.run(ymax, feed_dict={x: batchData, y: batchLabel, keepprob:1.})\n",
    "            refimg = testData[index, :, :, :].reshape(height, width, 3)\n",
    "            gtimg = yMaxOut[0, :, :].reshape(height, width)\n",
    "            errimg = gtimg - predMaxOut[0, :, :].reshape(height, width)\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 10)) \n",
    "            plt.subplot(2, 2, 1); plt.imshow(refimg); plt.title('Input')\n",
    "            plt.subplot(2, 2, 2); plt.pcolor(xs, ys, gtimg, vmin=0, vmax=nrclass);  plt.title('Ground truth')\n",
    "            plt.subplot(2, 2, 3); plt.pcolor(xs, ys, predMaxOut[0, :, :].reshape(height, width), vmin=0, vmax=nrclass); plt.title('[Validation] Prediction')\n",
    "            plt.subplot(2, 2, 4); plt.imshow(np.abs(errimg) > 0.5); plt.title('Error')\n",
    "            plt.show()\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
